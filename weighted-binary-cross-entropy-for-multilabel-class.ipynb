{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3\n",
    "\n",
    "For N labels in multi-label classification, it doesn't really matter whether you sum the loss for each class, or whether you compute the average loss using tf.reduce_mean: the gradient would point in the same direction.\n",
    "\n",
    "However, if you divide the sum by N (this is what averaging essentially is), this will influence the learning rate at the end of the day. If you are not sure how many labels in the multi-label classification task there will be, it might be easier to use tf.reduce_mean, because you wouldn't have to readjust the weight of this loss component compared to other components of the loss, and you wouldn't have to adjust the learning rate in the number N of labels changes.\n",
    "\n",
    "shareimprove this answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40198364/how-can-i-implement-a-weighted-cross-entropy-loss-in-tensorflow-using-sparse-sof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/35155655/loss-function-for-class-imbalanced-binary-classifier-in-tensor-flow/35168022#35168022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# let's say we have the logits and labels of a batch of size 6 with 5 classes\n",
    "logits = tf.constant(np.random.randint(0, 10, 30).reshape(6, 5), dtype=tf.float32)\n",
    "labels = tf.constant(np.random.randint(0, 5, 6), dtype=tf.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 6., 1., 3.],\n",
       "       [9., 6., 1., 0., 1.],\n",
       "       [9., 0., 0., 9., 3.],\n",
       "       [4., 0., 0., 4., 1.],\n",
       "       [7., 3., 2., 4., 7.],\n",
       "       [2., 4., 8., 0., 7.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "logits.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4, 4, 4, 1], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.eval ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify some class weightings\n",
    "class_weights = tf.constant([0.3, 0.1, 0.2, 0.3, 0.1])\n",
    "\n",
    "# specify the weights for each sample in the batch (without having to compute the onehot label matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.gather(class_weights, labels)\n",
    "\n",
    "# compute the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.3, 0.1, 0.1, 0.1, 0.1], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77875894"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.losses.sparse_softmax_cross_entropy(labels, logits, weights).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.ones(shape=[3,4])\n",
    "w=tf.constant([0.1,0.3,0.4,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.3, 0.4, 0.5],\n",
       "       [0.1, 0.3, 0.4, 0.5],\n",
       "       [0.1, 0.3, 0.4, 0.5]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x*w).eval()  #so multiplication operation gets broadcased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1, 1.3, 1.4, 1.5],\n",
       "       [1.1, 1.3, 1.4, 1.5],\n",
       "       [1.1, 1.3, 1.4, 1.5]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x+w).eval()  # so addition operation gets broadcased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary crossentropy\n",
    "x=tf.constant(np.random.uniform(0,1,size=[3,4]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6547213 , 0.37380144, 0.23451288, 0.98799527],\n",
       "       [0.765996  , 0.7770044 , 0.02798196, 0.17390652],\n",
       "       [0.15408224, 0.07708648, 0.8898657 , 0.7503787 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval() # assume these are probabilities predicted by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.constant([1,0,0,1],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=tf.stack([y,y,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.],\n",
       "       [1., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=y1,logits=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4184385 , 0.89741313, 0.8172624 , 0.31650445],\n",
       "       [0.38176697, 1.1552911 , 0.707236  , 0.6099696 ],\n",
       "       [0.61907077, 0.7324331 , 1.2339594 , 0.38674954]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.eval()  #losses for different classes class 1 , class2, class 3, class 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6547213 , 0.37380144, 0.23451288, 0.98799527],\n",
       "       [0.765996  , 0.7770044 , 0.02798196, 0.17390652],\n",
       "       [0.15408224, 0.07708648, 0.8898657 , 0.7503787 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if class2 is imbalanced\n",
    "# https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let say \n",
    "w=tf.constant([1,2,1,2],dtype=tf.float32)\n",
    "w=tf.reshape(w,shape=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1=loss*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4184385 , 1.7948263 , 0.8172624 , 0.6330089 ],\n",
       "       [0.38176697, 2.3105822 , 0.707236  , 1.2199392 ],\n",
       "       [0.61907077, 1.4648662 , 1.2339594 , 0.7734991 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4184385 , 0.89741313, 0.8172624 , 0.31650445],\n",
       "       [0.38176697, 1.1552911 , 0.707236  , 0.6099696 ],\n",
       "       [0.61907077, 0.7324331 , 1.2339594 , 0.38674954]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.eval() #so we have increased loss for skewed label classes which will force the model to spend more time on classificatin of class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "netloss=tf.reduce_mean(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0312047"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netloss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
